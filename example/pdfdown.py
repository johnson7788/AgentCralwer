#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Date  : 2025/10/11 09:34
# @File  : pdfdown.py.py
# @Author: johnson
# @Contact : github: johnson7788
# @Desc  :
import re
import logging
import asyncio
import aiohttp
import async_timeout
from tqdm import tqdm
import os
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig
from crawl4ai import AsyncWebCrawler

web_sites = [
  {
    "name": "åæ¶¦ç½®åœ°",
    "url": "https://crland-umb.azurewebsites.net/zh-cn/investors/financial-results-and-presentations/"
  },
  {
    "name": "ä¿¡è¾¾ç”Ÿç‰©",
    "url": "https://investor.innoventbio.com/cn/investors/webcasts-and-presentations/"
  },
  {
    "name": "ä½å‹å•†äº‹",
    "url": "https://www.sumitomocorp.com/ja/jp/ir/report"
  },
  {
    "name": "äº¿èˆªæ™ºèƒ½",
    "url": "https://ir.ehang.com/financial-information/quarterly-results"
  },
  {
    "name": "ç¢§æ¡‚å›­",
    "url": "https://www.countrygarden.com.cn/investor/presentation"
  },
  {
    "name": "èŠæ± åˆ¶ä½œæ‰€",
    "url": "https://www.kikuchiseisakusho.co.jp/ir/irnews.html"
  },
  {
    "name": "è‹±ä¼Ÿè¾¾",
    "url": "https://investor.nvidia.com/financial-info/financial-reports/default.aspx"
  },
  {
    "name": "ç‘å£°ç§‘æŠ€",
    "url": "https://www.aactechnologies.com/portfolio/investorsRelations"
  },
  {
    "name": "ç‰¹æ–¯æ‹‰",
    "url": "https://ir.tesla.com/#quarterly-disclosure"
  }
]
# æ¯é¡µä¸‹è½½2ä¸ªpdfæ–‡ä»¶ï¼Œç”¨äºæµ‹è¯•
LIMIT_NUM = 2

def get_run_configs(name):
    if name == "href_pdf":
        run_config = CrawlerRunConfig(
            js_code="""
                async function downloadFile(url, filename) {
                    const response = await fetch(url);
                    const blob = await response.blob();
                    const a = document.createElement('a');
                    a.href = window.URL.createObjectURL(blob);
                    a.download = filename || url.split('/').pop();
                    document.body.appendChild(a);
                    a.click();
                    a.remove();
                }

                const links = Array.from(
                  document.querySelectorAll(
                    'a[href$=".pdf"], a[href$=".PDF"], a[href*=".pdf?"], a[href*=".PDF?"]'
                  )
                );
                const firstTwo = links.slice(0, 2);
                for (const link of firstTwo) {
                    const url = link.href;
                    console.log("Downloading:", url);
                    await downloadFile(url);
                    await new Promise(r => setTimeout(r, 3000)); // ç­‰å¾…ä¸‹è½½è§¦å‘å®Œæˆ
                }
            """,
            # ç­‰å¾…ï¼šç¡®ä¿é¡µé¢ä¸Šè‡³å°‘å‡ºç°è¿‡ä¸€ä¸ª PDF é“¾æ¥ï¼ˆè€Œä¸æ˜¯ç­‰å›ºå®šç§’æ•°ï¼‰
            wait_for="css:a[href*='.pdf']",
            # ç»™ä¸‹è½½åŠ¨ä½œä¸€ç‚¹â€œæ”¶å°¾â€æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œä¸ä¼šè§¦å‘ .strip()
            delay_before_return_html=60,
            # å¯é€‰ï¼šæ‰©å¤§ç­‰å¾…ä¸Šé™ï¼ˆæ¯«ç§’ï¼‰
            wait_for_timeout=30000,
        )
    elif name == "application_pdf":
        run_config = CrawlerRunConfig(
            js_code="""
                async function downloadFile(url, filename) {
                    const response = await fetch(url);
                    const blob = await response.blob();
                    const a = document.createElement('a');
                    a.href = window.URL.createObjectURL(blob);
                    a.download = filename || url.split('/').pop() || "download.pdf";
                    document.body.appendChild(a);
                    a.click();
                    a.remove();
                }
    
                const links = Array.from(
                  document.querySelectorAll(
                    'a[href$=".pdf"], a[href$=".PDF"], a[href*=".pdf?"], a[href*=".PDF?"], a[type="application/pdf"]'
                  )
                );
                const firstTwo = links.slice(0, 2);
                for (const link of firstTwo) {
                    const url = link.href;
                    const filename = link.getAttribute('title') || 'file.pdf';
                    console.log("Downloading:", url, filename);
                    await downloadFile(url, filename);
                    await new Promise(r => setTimeout(r, 3000));
                }
            """,
            wait_for="css:a[href*='.pdf'], a[type='application/pdf']",
            delay_before_return_html=60,
            wait_for_timeout=30000,
        )
    return run_config

async def download_all_pdfs_from_url(run_config, url: str, download_path: str):
    os.makedirs(download_path, exist_ok=True)
    # é…ç½®å…è®¸ä¸‹è½½ & ä¸‹è½½ç›®å½•
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                      "AppleWebKit/537.36 (KHTML, like Gecko) "
                      "Chrome/118.0.0.0 Safari/537.36",
        "Referer": url,
        "Accept-Language": "en-US,en;q=0.9",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
    }
    config = BrowserConfig(
        accept_downloads=True,
        headless=False,
        downloads_path=download_path,
        headers=headers,
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                   "AppleWebKit/537.36 (KHTML, like Gecko) "
                   "Chrome/127.0.0.0 Safari/537.36",
        enable_stealth=True,  # ğŸš€ å¯ç”¨ playwright-stealth
    )
    async with AsyncWebCrawler(config=config) as crawler:
        result = await crawler.arun(url=url, config=run_config)
        # è¾“å‡ºä¸‹è½½ç»“æœ
        if result.downloaded_files:
            print("Downloaded files:")
            for f in result.downloaded_files:
                print(" -", f)
            return True
        else:
            print("No PDF files were downloaded.")
            return False

async def fetch_pdfs_from_page(url, download_dir):
    """
    å¯¹äºå¤–é“¾æƒ…å†µçš„å¤„ç†ï¼Œè‡ªåŠ¨é‡è¯•3æ¬¡
    :param url:
    :param download_dir:
    :return:
    """
    headers = {
        # ç»Ÿä¸€ UA + ç°ä»£æµè§ˆå™¨å¸¸è§å¤´ï¼Œå‡å°‘â€œåƒè„šæœ¬â€çš„ç‰¹å¾
        "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                       "AppleWebKit/537.36 (KHTML, like Gecko) "
                       "Chrome/127.0.0.0 Safari/537.36"),
        "Accept": ("text/html,application/xhtml+xml,application/xml;q=0.9,"
                   "image/avif,image/webp,*/*;q=0.8"),
        "Accept-Language": "en-US,en;q=0.9",
        "Upgrade-Insecure-Requests": "1",
    }
    os.makedirs(download_dir, exist_ok=True)
    config = BrowserConfig(headless=False, headers=headers)
    run_config = CrawlerRunConfig(wait_for="css:a[href*='.pdf']")
    # æ˜¯å¦æœ‰ä¸‹è½½æˆåŠŸçš„
    statuses = []
    async with AsyncWebCrawler(config=config) as crawler:
        result = await crawler.arun(url=url, config=run_config)
        html = result.html

        pdf_urls = re.findall(r'href="(https?://[^"]+\.pdf)"', html, re.IGNORECASE)
        pdf_urls = list(set(pdf_urls))

        print(f"å‘ç° {len(pdf_urls)} ä¸ª PDF é“¾æ¥")
        for link in pdf_urls[:LIMIT_NUM]:
            filename = link.split("/")[-1]
            save_path = os.path.join(download_dir, filename)
            print(f"ä¸‹è½½ {link} â†’ {filename}")
            for attempt in range(3):
                try:
                    async with aiohttp.ClientSession(headers=headers) as session:
                        async with async_timeout.timeout(60):  # è¶…æ—¶æ—¶é—´å»¶é•¿
                            async with session.get(link, ssl=False) as resp:
                                if resp.status == 200:
                                    with open(save_path, "wb") as f:
                                        f.write(await resp.read())
                                    print(f"âœ… ä¸‹è½½æˆåŠŸ: {filename}")
                                    statuses.append(True)
                                    break
                                else:
                                    print(f"âš ï¸ çŠ¶æ€ç  {resp.status}")
                                    statuses.append(False)
                    await asyncio.sleep(2)
                except Exception as e:
                    print(f"âŒ ç¬¬ {attempt + 1} æ¬¡ä¸‹è½½å‡ºé”™: {e}")
                    if attempt == 2:
                        statuses.append(False)
    status = all(statuses)
    return status

def download_one_site(one_site, select_method="all"):
    site_name = one_site["name"]
    site_url = one_site["url"]
    print(f"ä¸‹è½½ {site_name} ä¸­çš„PDFæ–‡ä»¶ ...")
    save_dir = os.path.join(SAVE_DIR, site_name)
    # ä¾æ¬¡å°è¯•ä¸åŒä¸‹è½½æ–¹å¼ï¼ŒæˆåŠŸåç«‹å³åœæ­¢
    success = False  # æ ‡è®°æ˜¯å¦å·²æˆåŠŸä¸‹è½½
    if select_method in ["all", "href_pdf", "application_pdf"]:
        for method in ["href_pdf", "application_pdf"]:
            if select_method != "all" and method != select_method:
                print(f"è·³è¿‡ {method} æ–¹æ³•ï¼Œå› ä¸ºå·²é€‰æ‹© {select_method} æ–¹æ³•ã€‚")
                continue
            print(f"ä½¿ç”¨ {method} æ–¹æ³•ä¸‹è½½ {site_name} ä¸­çš„PDFæ–‡ä»¶ ...")
            run_config = get_run_configs(name=method)
            status = asyncio.run(download_all_pdfs_from_url(run_config, site_url, save_dir))
            print(f"{site_name} ä½¿ç”¨ {method} ä¸‹è½½ç»“æœï¼š{status}")
            # å¦‚æœæˆåŠŸï¼ˆstatus ä¸º Trueï¼‰ï¼Œåˆ™åœæ­¢ç»§ç»­å°è¯•
            if status:
                print(f"{site_name} ä¸‹è½½æˆåŠŸï¼Œåœæ­¢å°è¯•å…¶ä»–æ–¹æ³•ã€‚")
                success = True
                break
            else:
                print(f"{site_name} ä½¿ç”¨ {method} æ–¹å¼æœªæˆåŠŸï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ–¹å¼ã€‚")
        else:
            # å¦‚æœæ‰€æœ‰æ–¹å¼éƒ½å¤±è´¥ï¼ˆforå¾ªç¯æœªbreakï¼‰ï¼Œæ‰“å°æç¤º
            print(f"âŒ {site_name} æ‰€æœ‰ä¸‹è½½æ–¹å¼å‡æœªæˆåŠŸã€‚")
    if select_method in ["all", "fetch_pdfs_by_url"]:
        # å¦‚æœæ‰€æœ‰æ–¹å¼éƒ½å¤±è´¥ï¼Œåˆ™å°è¯• fetch_pdfs_from_page
        if not success:
            print(f"å°è¯• fetch_pdfs_from_page ...")
            status = asyncio.run(fetch_pdfs_from_page(site_url, save_dir))
            print(f"{site_name} ä½¿ç”¨ fetch_pdfs_from_page ä¸‹è½½ç»“æœï¼š{status}")

def download_all_sites():
    for one_site in tqdm(web_sites, desc="ä¸‹è½½è¿›åº¦"):
        download_one_site(one_site)

if __name__ == "__main__":
    SAVE_DIR = "./downloaded_pdfs"
    # download_one_site(one_site = web_sites[-1], select_method="application_pdf")
    download_one_site(one_site = web_sites[-1], select_method="fetch_pdfs_by_url")
    # download_all_sites()
